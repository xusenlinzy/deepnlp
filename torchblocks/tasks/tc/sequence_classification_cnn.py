import torch
import torch.nn as nn
import torch.nn.functional as F


class TextCNN(nn.Module):
    """
    æ–‡æœ¬å·ç§¯ç½‘ç»œ
    + ğŸ“– é¦–å…ˆé€šè¿‡å·ç§¯æ ¸æå–å¤šç§`n-gram`ç‰¹å¾
    + ğŸ“– æœ€åé€šè¿‡æœ€å¤§æ± åŒ–æå–æ–‡æœ¬ä¸»è¦ç‰¹å¾
    
    Args:
        `num_labels`: ç±»åˆ«æ•°
        `n_filters`: å·ç§¯æ ¸ä¸ªæ•°ï¼ˆå¯¹åº”2ç»´å·ç§¯çš„é€šé“æ•°ï¼‰
        `filter_sizes`: å·ç§¯æ ¸çš„å°ºå¯¸(3, 4, 5)
    
    Reference:
        â­ï¸ [Convolutional Neural Networks for Sentence Classification.](https://arxiv.org/abs/1408.5882)
        ğŸš€ [Code](https://github.com/yoonkim/CNN_sentence)
    """

    def __init__(self, num_labels, hidden_size, n_filters=3, filter_sizes=(3, 4, 5)):
        super(TextCNN, self).__init__()
        self.convs = nn.ModuleList(
            [nn.Conv2d(in_channels=1, out_channels=n_filters,
                       kernel_size=(fs, hidden_size)) for fs in filter_sizes])
        self.classifier = nn.Linear(len(filter_sizes) * n_filters, num_labels)

    def forward(self, sequence_output):
        # sequence_output = [batch size, 1, sent len, emb dim]
        sequence_output = sequence_output.unsqueeze(1).float()
        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]
        conved = [F.relu(conv(sequence_output)).squeeze(3) for conv in self.convs]
        # pooled_n = [batch size, n_filters]
        pooled = [F.max_pool1d(conv, int(conv.shape[2])).squeeze(2) for conv in conved]
        cat = torch.cat(pooled, dim=1)  # [batch size, n_filters * len(filter_sizes)]
        return self.classifier(cat)


class DPCNN(nn.Module):
    """
    é‡‘å­—å¡”ç»“æ„çš„æ·±åº¦å·ç§¯ç½‘ç»œ
    + ğŸ“– `Region embedding`æå–æ–‡æœ¬çš„`n-gram`ç‰¹å¾
    + ğŸ“– ç­‰é•¿å·ç§¯+1/2æ± åŒ–å±‚è·å–æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    + ğŸ“– æ®‹å·®è¿æ¥é¿å…ç½‘ç»œé€€åŒ–ï¼Œä½¿å¾—æ·±å±‚ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒ
    
    Args:
        `num_labels`: ç±»åˆ«æ•°
        `n_filters`: å·ç§¯æ ¸ä¸ªæ•°ï¼ˆå¯¹åº”2ç»´å·ç§¯çš„é€šé“æ•°ï¼‰
    
    Reference:
        â­ï¸ [Deep Pyramid Convolutional Neural Networks for Text Categorization.](https://aclanthology.org/P17-1052.pdf)
        ğŸš€ [Code](https://github.com/649453932/Chinese-Text-Classification-Pytorch)
    """

    def __init__(self, num_labels, hidden_size, n_filters=3):
        super(DPCNN, self).__init__()
        self.conv_region = nn.Conv2d(1, n_filters, (3, hidden_size), stride=1)
        self.conv = nn.Conv2d(n_filters, n_filters, (3, 1), stride=1)

        self.max_pool = nn.MaxPool2d(kernel_size=(3, 1), stride=2)
        self.padding1 = nn.ZeroPad2d((0, 0, 1, 1))  # top bottom
        self.padding2 = nn.ZeroPad2d((0, 0, 0, 1))  # bottom

        self.relu = nn.ReLU()
        self.classifier = nn.Linear(n_filters, num_labels)

    def forward(self, sequence_output):
        # [batch size, 1, seq len, emb dim]
        x = sequence_output.unsqueeze(1).to(torch.float32)
        # [batch size, n_filters, seq len-3+1, 1]
        x = self.conv_region(x)
        # [batch size, n_filters, seq len, 1]
        x = self.padding1(x)
        x = self.relu(x)
        # [batch size, n_filters, seq len-3+1, 1]
        x = self.conv(x)
        # [batch size, n_filters, seq len, 1]
        x = self.padding1(x)
        x = self.relu(x)
        # [batch size, n_filters, seq len-3+1, 1]
        x = self.conv(x)
        # [batch size, n_filters, 1, 1]
        while x.size()[2] >= 2:
            x = self._block(x)
        # [batch size, n_filters]
        x = x.squeeze()
        return self.classifier(x)

    def _block(self, x):
        x = self.padding2(x)
        px = self.max_pool(x)

        x = self.padding1(px)
        x = F.relu(x)
        x = self.conv(x)

        x = self.padding1(x)
        x = F.relu(x)
        x = self.conv(x)

        # Short Cut
        x = x + px
        return x


class RCNN(nn.Module):
    """
    å¾ªç¯å·ç§¯ç½‘ç»œ
    + ğŸ“– `RNN`æ“…é•¿å¤„ç†åºåˆ—ç»“æ„ï¼Œèƒ½å¤Ÿè€ƒè™‘åˆ°å¥å­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†æ˜¯ä¸€ä¸ªä¸ªå¥å­ä¸­è¶Šå¾€åçš„è¯é‡è¦æ€§è¶Šé«˜ï¼Œæœ‰å¯èƒ½å½±å“æœ€åçš„åˆ†ç±»ç»“æœ
    + ğŸ“– `CNN`èƒ½å¤Ÿé€šè¿‡æœ€å¤§æ± åŒ–è·å¾—æœ€é‡è¦çš„ç‰¹å¾ï¼Œä½†æ˜¯æ»‘åŠ¨çª—å£å¤§å°ä¸å®¹æ˜“ç¡®å®šï¼Œé€‰çš„è¿‡å°å®¹æ˜“é€ æˆé‡è¦ä¿¡æ¯ä¸¢å¤±
    + ğŸ“– é¦–å…ˆç”¨åŒå‘å¾ªç¯ç»“æ„è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…¶æ¬¡ä½¿ç”¨æœ€å¤§æ± åŒ–å±‚è·å–æ–‡æœ¬çš„é‡è¦ç‰¹å¾
    
    Args:
        `num_labels`: ç±»åˆ«æ•°ç›®
        `hidden_size`: è¯åµŒå…¥ç»´åº¦
        `rnn_dim`: `rnn`éšè—å±‚çš„ç»´åº¦
        `rnn_layers`: `rnn`å±‚æ•°
        `rnn_type`: `rnn`ç±»å‹ï¼ŒåŒ…æ‹¬[`lstm`, `gru`, `rnn`]
        `bidirectional`: æ˜¯å¦åŒå‘
        `dropout`: `dropout`æ¦‚ç‡
        `batch_first`: ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¦æ˜¯æ‰¹é‡å¤§å°
    
    Reference:
        â­ï¸ [Recurrent convolutional neural networks for text classification.](https://dl.acm.org/doi/10.5555/2886521.2886636)
        ğŸš€ [Code](https://github.com/649453932/Chinese-Text-Classification-Pytorch)
    """

    def __init__(self, num_labels, hidden_size, rnn_dim, rnn_type='lstm', rnn_layers=1, bidirectional=True,
                 batch_first=True, dropout=0.2):
        super(RCNN, self).__init__()
        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(hidden_size,
                               rnn_dim,
                               num_layers=rnn_layers,
                               bidirectional=bidirectional,
                               batch_first=batch_first,
                               dropout=dropout)
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(hidden_size,
                              rnn_dim,
                              num_layers=rnn_layers,
                              bidirectional=bidirectional,
                              batch_first=batch_first,
                              dropout=dropout)
        else:
            self.rnn = nn.RNN(hidden_size,
                              rnn_dim,
                              num_layers=rnn_layers,
                              bidirectional=bidirectional,
                              batch_first=batch_first,
                              dropout=dropout)

        # 1 x 1 å·ç§¯ç­‰ä»·äºå…¨è¿æ¥å±‚ï¼Œæ•…æ­¤å¤„ä½¿ç”¨å…¨è¿æ¥å±‚ä»£æ›¿
        self.fc_cat = nn.Linear(rnn_dim * 2 + hidden_size, hidden_size)
        self.classifier = nn.Linear(hidden_size, num_labels)

    def forward(self, sequence_output):
        output, _ = self.rnn(sequence_output)
        seq_len = output.shape[1]
        # æ‹¼æ¥å·¦å³ä¸Šä¸‹æ–‡ä¿¡æ¯
        output = torch.tanh(self.fc_cat(torch.cat((output, sequence_output), dim=2)))
        output = torch.transpose(output, 1, 2)
        # æœ€å¤§æ± åŒ–
        output = F.max_pool1d(output, int(seq_len)).squeeze().contiguous()
        return self.classifier(output)


class TextRNN(nn.Module):
    """
    å¾ªç¯ç¥ç»ç½‘ç»œ
    
    Args:
        `num_labels`: ç±»åˆ«æ•°ç›®
        `hidden_size`: è¯åµŒå…¥ç»´åº¦
        `rnn_dim`: `rnn`éšè—å±‚çš„ç»´åº¦
        `rnn_layers`: `rnn`å±‚æ•°
        `rnn_type`: `rnn`ç±»å‹ï¼ŒåŒ…æ‹¬[`lstm`, `gru`, `rnn`]
        `bidirectional`: æ˜¯å¦åŒå‘
        `dropout`: `dropout`æ¦‚ç‡
        `batch_first`: ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¦æ˜¯æ‰¹é‡å¤§å°
        `attention`: æ˜¯å¦å¢åŠ æ³¨æ„åŠ›æœºåˆ¶
    
    Reference:
        ğŸš€ [Code](https://github.com/649453932/Chinese-Text-Classification-Pytorch)
    """

    def __init__(self, num_labels, hidden_size, rnn_dim, rnn_type='lstm', rnn_layers=1, bidirectional=True,
                 batch_first=True, dropout=0.2, attention=True):
        super(TextRNN, self).__init__()
        self.attention = attention
        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(hidden_size,
                               rnn_dim,
                               num_layers=rnn_layers,
                               bidirectional=bidirectional,
                               batch_first=batch_first,
                               dropout=dropout)
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(hidden_size,
                              rnn_dim,
                              num_layers=rnn_layers,
                              bidirectional=bidirectional,
                              batch_first=batch_first,
                              dropout=dropout)
        else:
            self.rnn = nn.RNN(hidden_size,
                              rnn_dim,
                              num_layers=rnn_layers,
                              bidirectional=bidirectional,
                              batch_first=batch_first,
                              dropout=dropout)
        # queryå‘é‡
        if self.attention:
            self.u = nn.Parameter(torch.randn(rnn_dim * 2), requires_grad=True)
            self.tanh = nn.Tanh()

        self.fc = nn.Linear(rnn_dim * 2, num_labels)
        self.dropout = nn.Dropout(dropout)

    def forward(self, sequence_output):
        output, _ = self.rnn(sequence_output)

        if self.attention:
            alpha = F.softmax(torch.matmul(self.tanh(output), self.u), dim=1).unsqueeze(-1)
            output = output * alpha  # [batch_size, seq_len, rnn_dim * num_directionns ]
            output = torch.sum(output, dim=1)  # [batch_size, hidden_dim]
            output = self.dropout(output)
        return self.fc(output)
