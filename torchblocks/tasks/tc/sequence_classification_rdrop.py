import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from typing import Optional
from ..utils import SequenceClassifierOutput, MODEL_MAP


def get_auto_rdrop_tc_model(model_type: str = "bert"):
    base_model, parent_model = MODEL_MAP[model_type]
    
    class BertWithRDrop(parent_model):
        """
        + ðŸ“– é€šè¿‡å¢žåŠ ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œæ¥å¼ºåŒ–æ¨¡åž‹å¯¹`Dropout`çš„é²æ£’æ€§
        + ðŸ“– ä½¿å¾—ä¸åŒçš„`Dropout`ä¸‹æ¨¡åž‹çš„è¾“å‡ºåŸºæœ¬ä¸€è‡´ï¼Œå› æ­¤èƒ½é™ä½Žè¿™ç§ä¸ä¸€è‡´æ€§
        + ðŸ“– ä¿ƒè¿›â€œæ¨¡åž‹å¹³å‡â€ä¸Žâ€œæƒé‡å¹³å‡â€çš„ç›¸ä¼¼æ€§ï¼Œä½¿å¾—ç®€å•å…³é—­`Dropout`çš„æ•ˆæžœç­‰ä»·äºŽå¤š`Dropout`æ¨¡åž‹èžåˆçš„ç»“æžœ
        
        Args:
            `config`: æ¨¡åž‹çš„é…ç½®å¯¹è±¡
            
        Reference:
            â­ï¸ [R-Drop: Regularized Dropout for Neural Networks.](https://spaces.ac.cn/archives/8496) 
            ðŸš€ [Official Code](https://github.com/dropreg/R-Drop)
        """
        def __init__(self, config):
            super(BertWithRDrop, self).__init__(config)
            self.num_labels = config.num_labels
            self.bert = base_model(config)
            config.hidden_dropout_prob = 0.3
            self.config = config
            
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
            self.classifier = nn.Linear(config.hidden_size, config.num_labels)
            
            if hasattr(config, 'use_task_id') and config.use_task_id and model_type == "ernie":
                # Add task type embedding to BERT
                task_type_embeddings = nn.Embedding(config.task_type_vocab_size, config.hidden_size)
                self.bert.embeddings.task_type_embeddings = task_type_embeddings

                def hook(module, input, output):
                    return output + task_type_embeddings(
                        torch.zeros(input[0].size(), dtype=torch.int64, device=input[0].device))

                self.bert.embeddings.word_embeddings.register_forward_hook(hook)

            # Initialize weights and apply final processing
            self.post_init()

        def forward(
            self, 
            input_ids: Optional[torch.Tensor] = None, 
            attention_mask: Optional[torch.Tensor] = None,
            token_type_ids: Optional[torch.Tensor] = None,
            labels: Optional[torch.Tensor] = None,
        ) -> SequenceClassifierOutput:

            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
            pooled_output = outputs['pooler_output']
            logits = self.dropout(self.classifier(pooled_output))

            if self.training:
                outputs2 = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
                pooled_output2 = outputs2['pooler_output']
                logits2 = self.dropout(self.classifier(pooled_output2))

            loss = None
            if labels is not None:
                loss_fct = CrossEntropyLoss()
                alpha = getattr(self.config, "alpha", 4.0)
                loss = alpha * loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
                if self.training:
                    loss += alpha * loss_fct(logits2.view(-1, self.num_labels), labels.view(-1))

                    p = torch.log_softmax(logits[0].view(-1, self.num_labels), dim=-1)
                    p_tec = torch.softmax(logits[0].view(-1, self.num_labels), dim=-1)
                    q = torch.log_softmax(logits2[-1].view(-1, self.num_labels), dim=-1)
                    q_tec = torch.softmax(logits2[-1].view(-1, self.num_labels), dim=-1)

                    kl_loss = torch.nn.functional.kl_div(p, q_tec, reduction='none').sum()
                    reverse_kl_loss = torch.nn.functional.kl_div(q, p_tec, reduction='none').sum()
                    loss += 0.5 * (kl_loss + reverse_kl_loss) / 2

            return SequenceClassifierOutput(
                loss=loss,
                logits=logits,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )
            
    return BertWithRDrop
