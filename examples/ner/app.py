import sys

sys.path.append("../..")

import streamlit as st
from transformers import BertTokenizerFast
from torchblocks.tasks.ner import get_auto_ner_model
from torchblocks.tasks.ner import NERPredictor, EnsembleNERPredictor, PromptNERPredictor, LearNERPredictor, \
    W2NERPredictor

MODEL_PATH_MAP = {
    "tplinker": '/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/tplinkerplus/cmeee-tplinkerplus_bert_v0/checkpoint-eval_f1_micro-best',
    "crf": '/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/crf/cmeee-crf_bert_v0/checkpoint-eval_f1_micro-best',
    "span": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/span/cmeee-span_bert_v0/checkpoint-eval_f1_micro-best",
    "global-pointer": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/efficient-global-pointer/cmeee-efficient-global-pointer_bert_v0/checkpoint-eval_f1_micro-best",
    "w2ner": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/w2ner/cmeee-w2ner_bert_v0/checkpoint-eval_f1_micro-best",
    "lear": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/lear/cmeee-lear_bert_v0/checkpoint-eval_f1_micro-best",
    "mrc": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/mrc-prompt/cmeee-mrc_bert_v0/checkpoint-eval_f1_micro-best",
}

schema2prompt = {
    "dis": "ç–¾ç—…ï¼Œä¸»è¦åŒ…æ‹¬ç–¾ç—…ã€ä¸­æ¯’æˆ–å—ä¼¤å’Œå™¨å®˜æˆ–ç»†èƒå—æŸ",
    "sym": "ä¸´åºŠè¡¨ç°ï¼Œä¸»è¦åŒ…æ‹¬ç—‡çŠ¶å’Œä½“å¾",
    "pro": "åŒ»ç–—ç¨‹åºï¼Œä¸»è¦åŒ…æ‹¬æ£€æŸ¥ç¨‹åºã€æ²»ç–—æˆ–é¢„é˜²ç¨‹åº",
    "equ": "åŒ»ç–—è®¾å¤‡ï¼Œä¸»è¦åŒ…æ‹¬æ£€æŸ¥è®¾å¤‡å’Œæ²»ç–—è®¾å¤‡",
    "dru": "è¯ç‰©ï¼Œæ˜¯ç”¨ä»¥é¢„é˜²ã€æ²»ç–—åŠè¯Šæ–­ç–¾ç—…çš„ç‰©è´¨",
    "ite": "åŒ»å­¦æ£€éªŒé¡¹ç›®ï¼Œæ˜¯å–è‡ªäººä½“çš„ææ–™è¿›è¡Œè¡€æ¶²å­¦ã€ç»†èƒå­¦ç­‰æ–¹é¢çš„æ£€éªŒ",
    "bod": "èº«ä½“ï¼Œä¸»è¦åŒ…æ‹¬èº«ä½“ç‰©è´¨å’Œèº«ä½“éƒ¨ä½",
    "dep": "éƒ¨é—¨ç§‘å®¤ï¼ŒåŒ»é™¢çš„å„èŒèƒ½ç§‘å®¤",
    "mic": "å¾®ç”Ÿç‰©ç±»ï¼Œä¸€èˆ¬æ˜¯æŒ‡ç»†èŒã€ç—…æ¯’ã€çœŸèŒã€æ”¯åŸä½“ã€è¡£åŸä½“ã€èºæ—‹ä½“ç­‰å…«ç±»å¾®ç”Ÿç‰©"
}

# Using object notation
model_name = st.sidebar.radio(
    "æ¨¡å‹æ¡†æ¶",
    ("CRF", "SPAN", "TPLINKER", "GLOBAL-POINTER", "MRC", "LEAR", "W2NER", "ENSEMBLE")
)
st.sidebar.markdown('---')
max_seqlen = st.sidebar.number_input('å¥å­æœ€å¤§é•¿åº¦', 0, 512, 512)
prob = st.sidebar.slider("é˜ˆå€¼", min_value=0.0, max_value=1.0, value=0.5, step=0.01)


@st.cache(hash_funcs={BertTokenizerFast: id})
def load_tokenizer():
    return BertTokenizerFast.from_pretrained('hfl/chinese-roberta-wwm-ext', do_lower_case=True)


PREDICTOR_MAP = {"lear": LearNERPredictor, "w2ner": W2NERPredictor, "mrc": PromptNERPredictor}


def load_auto_predictor(model_name):
    predictor_class = PREDICTOR_MAP.get(model_name, NERPredictor)

    @st.cache(hash_funcs={predictor_class: id})
    def load_predictor():
        tokenizer = load_tokenizer()
        model_name_or_path = MODEL_PATH_MAP[model_name]
        model = get_auto_ner_model(model_name=model_name, model_type="bert")

        if model_name in ["lear", "mrc"]:
            return predictor_class(schema2prompt, model=model, model_name_or_path=model_name_or_path,
                                   tokenizer=tokenizer)
        else:
            return predictor_class(model, model_name_or_path, tokenizer)

    return load_predictor()


@st.cache(hash_funcs={EnsembleNERPredictor: id})
def load_ensemble_predictor():
    predictors = [load_auto_predictor(name) for name in
                  ["crf", "span", "global-pointer", "tplinker", "mrc", "lear", "w2ner"]]
    return EnsembleNERPredictor(predictors)


html_tmp = """
    <div>
    <h1 style="text-align:center;">ä¸­æ–‡åŒ»å­¦æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«</h1>
    </div>
"""
st.markdown(html_tmp, unsafe_allow_html=True)
st.markdown('---')

st.subheader("è¾“å…¥æ–‡æœ¬ğŸ“–")
text = st.text_area("è¯·è¾“å…¥å¾…æŠ½å–çš„å¥å­ï¼ˆæ”¯æŒå¤šä¸ªå¥å­è¾“å…¥ï¼‰ï¼š")

if model_name == "ENSEMBLE":
    ner = load_ensemble_predictor()
else:
    ner = load_auto_predictor(model_name.lower())

if st.button('è¿è¡ŒğŸš€'):
    text = text.split('\n')
    if model_name == "ENSEMBLE":
        out = ner.predict(text, max_length=max_seqlen, threshold=prob)
    else:
        out = ner.predict(text, max_length=max_seqlen)
    st.json(out)
    st.stop()
