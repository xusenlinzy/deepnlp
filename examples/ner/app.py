import sys

sys.path.append("../..")

import time
import streamlit as st
import pandas as pd
import seaborn as sns
from transformers import BertTokenizerFast
from torchblocks.tasks.ner import get_auto_ner_model
from torchblocks.tasks.ner import NERPredictor, EnsembleNERPredictor, PromptNERPredictor, LearNERPredictor, \
    W2NERPredictor
from torchblocks.utils.app import visualize_ner, download_button, _max_width_, make_color_palette

MODEL_PATH_MAP = {
    "tplinker": '/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/tplinkerplus/cmeee-tplinkerplus_bert_v0/checkpoint-eval_f1_micro-best',
    "crf": '/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/crf/cmeee-crf_bert_v0/checkpoint-eval_f1_micro-best',
    "span": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/span/cmeee-span_bert_v0/checkpoint-eval_f1_micro-best",
    "global-pointer": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/efficient-global-pointer/cmeee-efficient-global-pointer_bert_v0/checkpoint-eval_f1_micro-best",
    "w2ner": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/w2ner/cmeee-w2ner_bert_v0/checkpoint-eval_f1_micro-best",
    "lear": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/lear/cmeee-lear_bert_v0/checkpoint-eval_f1_micro-best",
    "mrc": "/home/xusenlin/nlp/deepnlp/examples/ner/outputs/cmeee/mrc-prompt/cmeee-mrc_bert_v0/checkpoint-eval_f1_micro-best",
}

schema2prompt = {
    "dis": "ç–¾ç—…ï¼Œä¸»è¦åŒ…æ‹¬ç–¾ç—…ã€ä¸­æ¯’æˆ–å—ä¼¤å’Œå™¨å®˜æˆ–ç»†èƒå—æŸ",
    "sym": "ä¸´åºŠè¡¨ç°ï¼Œä¸»è¦åŒ…æ‹¬ç—‡çŠ¶å’Œä½“å¾",
    "pro": "åŒ»ç–—ç¨‹åºï¼Œä¸»è¦åŒ…æ‹¬æ£€æŸ¥ç¨‹åºã€æ²»ç–—æˆ–é¢„é˜²ç¨‹åº",
    "equ": "åŒ»ç–—è®¾å¤‡ï¼Œä¸»è¦åŒ…æ‹¬æ£€æŸ¥è®¾å¤‡å’Œæ²»ç–—è®¾å¤‡",
    "dru": "è¯ç‰©ï¼Œæ˜¯ç”¨ä»¥é¢„é˜²ã€æ²»ç–—åŠè¯Šæ–­ç–¾ç—…çš„ç‰©è´¨",
    "ite": "åŒ»å­¦æ£€éªŒé¡¹ç›®ï¼Œæ˜¯å–è‡ªäººä½“çš„ææ–™è¿›è¡Œè¡€æ¶²å­¦ã€ç»†èƒå­¦ç­‰æ–¹é¢çš„æ£€éªŒ",
    "bod": "èº«ä½“ï¼Œä¸»è¦åŒ…æ‹¬èº«ä½“ç‰©è´¨å’Œèº«ä½“éƒ¨ä½",
    "dep": "éƒ¨é—¨ç§‘å®¤ï¼ŒåŒ»é™¢çš„å„èŒèƒ½ç§‘å®¤",
    "mic": "å¾®ç”Ÿç‰©ç±»ï¼Œä¸€èˆ¬æ˜¯æŒ‡ç»†èŒã€ç—…æ¯’ã€çœŸèŒã€æ”¯åŸä½“ã€è¡£åŸä½“ã€èºæ—‹ä½“ç­‰å…«ç±»å¾®ç”Ÿç‰©"
}

LABEL_MAP = {
    "dis": "ç–¾ç—…",
    "sym": "ç—‡çŠ¶ã€ä¸´åºŠè¡¨ç°",
    "pro": "æ£€æŸ¥ã€æ²»ç–—æˆ–é¢„é˜²ç¨‹åº",
    "equ": "æ£€æŸ¥è®¾å¤‡å’Œæ²»ç–—è®¾å¤‡",
    "dru": "è¯ç‰©",
    "ite": "åŒ»å­¦æ£€éªŒé¡¹ç›®",
    "bod": "èº«ä½“ç‰©è´¨å’Œèº«ä½“éƒ¨ä½",
    "dep": "éƒ¨é—¨ç§‘å®¤",
    "mic": "å¾®ç”Ÿç‰©"
}


@st.cache(hash_funcs={BertTokenizerFast: id})
def load_tokenizer():
    return BertTokenizerFast.from_pretrained('hfl/chinese-roberta-wwm-ext', do_lower_case=True)


PREDICTOR_MAP = {"lear": LearNERPredictor, "w2ner": W2NERPredictor, "mrc": PromptNERPredictor}
labels = LABEL_MAP.values()
colors = make_color_palette(labels)


def load_auto_predictor(model_name):
    predictor_class = PREDICTOR_MAP.get(model_name, NERPredictor)

    @st.cache(hash_funcs={predictor_class: id})
    def load_predictor():
        tokenizer = load_tokenizer()
        model_name_or_path = MODEL_PATH_MAP[model_name]
        model = get_auto_ner_model(model_name=model_name, model_type="bert")

        if model_name in ["lear", "mrc"]:
            return predictor_class(schema2prompt, model=model, model_name_or_path=model_name_or_path,
                                   tokenizer=tokenizer)
        else:
            return predictor_class(model, model_name_or_path, tokenizer)

    return load_predictor()


@st.cache(hash_funcs={EnsembleNERPredictor: id})
def load_ensemble_predictor():
    predictors = [load_auto_predictor(name) for name in
                  ["crf", "span", "global-pointer", "tplinker", "mrc", "lear", "w2ner"]]
    return EnsembleNERPredictor(predictors)


# è®¾ç½®ç½‘é¡µä¿¡æ¯ 
st.set_page_config(page_title="NER Demo", page_icon="ğŸš€", layout="wide")

_max_width_()

c30, c31, c32 = st.columns([2.5, 1, 3])

with c30:
    # st.image("logo.png", width=400)
    st.title("ğŸ”‘ ä¸­æ–‡åŒ»å­¦å‘½åå®ä½“è¯†åˆ«")
    st.header("")

with st.expander("â„¹ï¸ - å…³äºæ­¤APP", expanded=True):
    st.write(
        """     
-   å®ç°å¤šç§`NER`æ¨¡å‹æŠ½å–ä¸­æ–‡åŒ»å­¦æ–‡æœ¬ä¸­çš„å®ä½“ã€‚
-   åŒ…å«7ç§`SOTA`æ¨¡å‹ä»¥åŠé¢å¤–çš„ä¸€ä¸ªé›†æˆæ¨¡å‹ã€‚
	    """
    )

    st.markdown("")

st.markdown("")
st.markdown("## ğŸ“Œ è¾“å…¥")

with st.form(key="my_form"):
    ce, c1, _, c2, c3 = st.columns([0.07, 1, 0.07, 5, 0.07])

    with c1:
        model_name = st.radio(
            "é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹",
            ("CRF", "SPAN", "TPLINKER", "GLOBAL-POINTER", "MRC", "LEAR", "W2NER", "ENSEMBLE"),
            help="ç›®å‰æ”¯æŒä»¥ä¸Šå…«ä¸ªæ¨¡å‹ã€‚",
        )

        max_seq_len = st.number_input(
            'å¥å­æœ€å¤§é•¿åº¦',
            0,
            512,
            512,
            help="æ¨¡å‹è¾“å…¥çš„æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼Œè¶…è¿‡è¯¥é•¿åº¦åˆ™æˆªæ–­ã€‚")

        prob = st.slider(
            "é˜ˆå€¼",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.01,
            help="æ¨¡å‹è¾“å‡ºå®ä½“çš„é˜ˆå€¼ï¼Œå½“æ¦‚ç‡å€¼å¤§äºè¯¥å€¼åˆ™è¾“å‡ºè¯¥å®ä½“ï¼Œä»…å¯¹äº`ensemble`æ¨¡å‹ã€‚")

    with c2:
        text = st.text_area(
            "ğŸ“–è¯·è¾“å…¥å¾…æŠ½å–çš„å¥å­",
            height=400, )

        file_upload_exp = st.expander("ä¸Šä¼ æ–‡ä»¶")
        uploaded_file = file_upload_exp.file_uploader("Choose a file", type=".jsonl")
        submit_button = st.form_submit_button(label="âœ¨ è¿è¡Œ")

if not submit_button:
    st.stop()

if model_name == "ENSEMBLE":
    ner = load_ensemble_predictor()
else:
    ner = load_auto_predictor(model_name.lower())

if uploaded_file is not None:
    data = pd.read_json(uploaded_file, lines=True, encoding='utf-8')
    texts = data.text.values
    bar = st.progress(0.0)
    res = []
    for i, text in enumerate(texts):
        if model_name == "ENSEMBLE":
            rlt = ner.predict(text, max_length=max_seq_len, threshold=prob)
        else:
            rlt = ner.predict(text, max_length=max_seq_len)

        if model_name == "ENSEMBLE":
            rlt = {LABEL_MAP[_type]: [
                {"text": ent["text"], "start": ent["start"], "end": ent["end"],
                 "probability": float(ent["probability"])}
                for ent in ents] for _type, ents in rlt.items()}
        else:
            rlt = {LABEL_MAP[_type]: [
                {"text": ent["text"], "start": ent["start"], "end": ent["end"]}
                for ent in ents] for _type, ents in rlt.items()}

        bar.progress((i + 1) / len(texts))
        res.append(rlt)

        if i == 0:
            visualize_ner(text, [rlt], colors=colors)
    data["prediction"] = res
    CSVButton1 = download_button(data, "medical_predict.json", "ğŸ“¥ Download (.json)")
    st.stop()

start = time.time()
if model_name == "ENSEMBLE":
    rlt = ner.predict(text.split(), max_length=max_seq_len, threshold=prob)
else:
    rlt = ner.predict(text.split(), max_length=max_seq_len)
running_time = time.time() - start

res = []
for r in rlt:
    if model_name == "ENSEMBLE":
        tmp = {LABEL_MAP[_type]: [
            {"text": ent["text"], "start": ent["start"], "end": ent["end"], "probability": float(ent["probability"])}
            for ent in ents] for _type, ents in r.items()}
    else:
        tmp = {LABEL_MAP[_type]: [
            {"text": ent["text"], "start": ent["start"], "end": ent["end"]}
            for ent in ents] for _type, ents in r.items()}
    res.append(tmp)

st.markdown("## ğŸˆ ç»“æœå±•ç¤º")
st.header("")

cs, c1, c2, c3, cLast = st.columns([2, 1.5, 1.5, 1.5, 2])

with c1:
    CSVButton2 = download_button(res, "Data.csv", "ğŸ“¥ Download (.csv)")
with c2:
    CSVButton3 = download_button(res, "Data.txt", "ğŸ“¥ Download (.txt)")
with c3:
    CSVButton4 = download_button(res, "Data.json", "ğŸ“¥ Download (.json)")

c1, c2, c3 = st.columns([1, 3, 1])

with c2:
    st.info(f'è¿è¡Œæ—¶é—´ï¼š{int(running_time * 1000)} ms', icon="âœ…")
    visualize_ner(text, res, colors=colors)

    json_doc_exp = st.expander("JSON")
    json_doc_exp.json(res)

    dataframe_exp = st.expander("DATAFRAME")
    columns = ["text", "start", "end", "label", "probability"]
    for r in res:
        if model_name == "ENSEMBLE":
            data = [[t[columns[0]], t[columns[1]], t[columns[2]], k, t[columns[4]]] for k, v in r.items() for t in v]
            df = pd.DataFrame(data, columns=columns).sort_values(by="probability", ascending=False).reset_index(
                drop=True)
        else:
            data = [[t[columns[0]], t[columns[1]], t[columns[2]], k] for k, v in r.items() for t in v]
            df = pd.DataFrame(data, columns=columns[:4])
        df.index += 1

        if model_name == "ENSEMBLE":
            # Add styling
            cmGreen = sns.light_palette("green", as_cmap=True)
            cmRed = sns.light_palette("red", as_cmap=True)
            df = df.style.background_gradient(
                cmap=cmGreen,
                subset=[
                    "probability",
                ],
            )
            format_dictionary = {
                "probability": "{:.2%}",
            }

            df = df.format(format_dictionary)
        dataframe_exp.table(df)
